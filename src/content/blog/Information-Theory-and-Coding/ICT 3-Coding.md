---
author: Nan Lin
pubDatetime: 2025-01-03T08:00:00Z
modDatetime: 2025-01-03T08:00:00.000Z
title: ICT 3-Coding
slug: information-and-coding-theory-3
featured: false
draft: false
tags:
  - Information
  - Theory
description: Note of Course ICE4411P-Information Theory and Coding
---
## Table of contents

Friday, January 3, 2025

## Encoder

### Definition of the Source Encoder

The purpose of the **source encoder** is to transform a kind of symbol sequence into another type of symbol sequence (named **code sequence**):
- The symbol sequence generated by the source is $X^N = (X_{1} \dots X_{N})$
- Each of the symbol is an element of the set $\{ x_{1}, x_{2}, \dots,x_{q}\}$ where the size of the set is $q$.
- _The encoder transform the symbol sequence into a code sequence:_ $X^N \to C^L = (C_{1}\dots C_{L})$
- Each of the symbol of the code sequence is an element of the set $\{ c_{1}, c_{2}\dots, c_{r}\}$, where the size of the set is $r$.

$$
\begin{bmatrix}
X^N = (X_{1}\dots X_{N}) \\
X_{i} \in \{x_{1}, \dots, x_{q}\} \\
\text{Possibilities: } q^N
\end{bmatrix} \to
\begin{bmatrix}
C^N = (C_{1}\dots C_{L}) \\
C_{i} \in \{c_{1}, \dots, c_{r}\} \\
\text{Possibilities: } r^L
\end{bmatrix} 
$$

> *Example*: transform decimal numbers to binary numbers, 4 -> 100. 

> *Example*: transform alphabets into digits, AB -> 12.

### Category of Encoding: Lossless vs Effective Encoding



## Lossless Encoding


### Evaluation Metric: Compress Ratio

**Compress Ratio**: Evaluate in what extent the data is compressed. Denote $L_B$ as the length of the source sequence and $L_D$ as the length of the encoded sequence.
$$
P_{r} = \frac{L_{B} - L_{D}}{L_{B}} \times 100 \%
$$

**Average code length**: Evaluate the average code length per source symbol.
- Lossless encoding, each $x_i$ corresponds $c_i$ with same distribution probability.
- Thus, the code length is the mathematical expectation of the new code length.

Note the length of each code symbol $l_i = l(c_i)$ where $c_i$ is the encoded code symbol of source symbol $x_i$. 

Case of single-symbol:
$$
\bar{L} = \mathbb{E}_{X}[l_{i}] =  \sum_{x_{i}} l(c_{i}) p(x_{i})
$$

Case of multiple-symbol: (Here, $\alpha_{i} = (x_{i_{1}}x_{i_{2}}\dots x_{iN})$)
$$
\overline{L_{N}} = \mathbb{E}_{X^N}[l(\alpha_{i})] = \sum_{\alpha_{i}} l(c_{i}) p (\alpha_{i})
$$

Thus, we have **average code length per symbol of a (source) sequence**:
$$
\bar{L} = \frac{\overline{L_{N}}}{N}
$$

### Evaluation Metric: Coding Information Rate

**Coding information rate**: Evaluate the average information _of the source_ of each _code_ symbol
> Note: Avg SOURCE Information per CODE symbol

Case of single-symbol:
$$
R = \frac{H(X)}{\bar{L}}
$$

Recap: The entropy, i.e. expectation of information of a sequence is $H(X^N)$ and the average symbol entropy of a memoryless source is $H_{N}(X)$.

Case of multiple-symbol: it is simultaneously ...
- (i) the entropy of the (source) sequence divided by the length of the encoded sequence.
- (ii) the avg entropy per symbol of the (source) sequence divided by the avg length of each symbol of the encoded sequence.
$$
R = \frac{H(X^N)}{\overline{L_{N}}} = \frac{H_{N}(X)}{\bar{L}}
$$
### Evaluation Metric: Coding Efficiency and Channel Redundancy

> Note: To calculate the efficiency, we need to compare the entropy with the maximum transmitted information rate, which is achieved when sending equiv-distributed encoded sequence.

**Capacity of the channel (maximum information rate) necessary to transmit a single source symbol**: Evaluate the maximum transmitted information  _by the encoded symbol/sequence_  of each *source* symbol
- We want to evaluate the maximum transmission capacity of the channel
- In this case, the probability distribution of the _code symbol_ should be equivalent (_Maximum discrete theorem_)
- Under this condition, observing the useful information actually transmitted per _source_ symbol can evaluate its efficiency.
> Note: Max CODE Information per SOURCE symbol.
> Note: The term "Necessary" here means the minimum requirement of the transmission capacity of the channel (i.e. maximum information rate)

The maximum transmission rate that could be reached when the code symbols are equal-distributed:
$$
R_{\max} = \frac{\max_{p(c_{i})} H(C^L)}{N} = \frac{\overline{L_{N} }}{N} \log r = \bar{L} \log r
$$

The coding efficiency is comparing the actual encoded information rate to the theoretical maximum transmission rate:
$$
\eta = \frac{H_{N}(X)}{R_{\max}} = \frac{H_{N}(X)}{\bar{L}\log r}
$$

**Channel redundancy** refers to the extent to which a channel's capacity is **not fully utilized** to transmit useful information.
$$
\gamma = 1 - \eta
$$






